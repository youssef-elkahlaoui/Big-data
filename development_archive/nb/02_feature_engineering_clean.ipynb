{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762e22d3",
   "metadata": {},
   "source": [
    "# Feature Engineering for Food Recommendation System\n",
    "\n",
    "This notebook performs feature engineering on the cleaned Open Food Facts dataset using PySpark for scalable processing.\n",
    "\n",
    "## Data Sources:\n",
    "- `../data/cleaned_food_data_filtered.csv` - Original cleaned food data\n",
    "- `../data/engineered_features_filtered.csv` - Previously engineered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d34834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StandardScaler as SparkScaler,\n",
    "    HashingTF, IDF, Tokenizer, StopWordsRemover,\n",
    "    StringIndexer, OneHotEncoder, Bucketizer\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import (\n",
    "    udf, col, when, isnan, isnull,\n",
    "    regexp_replace, lower, trim, size\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PySpark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FoodRecommendationFeatureEngineering\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark session created! Version: {spark.version}\")\n",
    "\n",
    "# Load data\n",
    "df_spark = spark.read.csv('../data/cleaned_food_data_filtered.csv', \n",
    "                         header=True, inferSchema=True)\n",
    "print(f\"Data loaded: {df_spark.count():,} rows, {len(df_spark.columns)} columns\")\n",
    "\n",
    "# Show basic info\n",
    "print(\"\\nData Schema:\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd99bc7",
   "metadata": {},
   "source": [
    "## Text Feature Engineering\n",
    "\n",
    "Create features from text fields (ingredients, categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_features(df):\n",
    "    \"\"\"Create text-based features\"\"\"\n",
    "    print(\"Creating text features...\")\n",
    "    \n",
    "    # Clean ingredients text\n",
    "    if 'ingredients_text' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'ingredients_filtered',\n",
    "            F.regexp_replace(F.lower(F.col('ingredients_text')), r'[^a-zA-Z\\s,]', '')\n",
    "        )\n",
    "        \n",
    "        # Count ingredients\n",
    "        df = df.withColumn(\n",
    "            'ingredient_count',\n",
    "            F.size(F.split(F.col('ingredients_filtered'), ','))\n",
    "        )\n",
    "        \n",
    "        # Allergen flags\n",
    "        allergens = ['gluten', 'milk', 'eggs', 'nuts', 'peanuts', 'soy']\n",
    "        for allergen in allergens:\n",
    "            df = df.withColumn(\n",
    "                f'contains_{allergen}',\n",
    "                F.when(F.col('ingredients_filtered').contains(allergen), 1).otherwise(0)\n",
    "            )\n",
    "    \n",
    "    # Category processing\n",
    "    if 'main_category' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'category_count',\n",
    "            F.size(F.split(F.col('main_category'), ','))\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply text features\n",
    "df_with_text = create_text_features(df_spark)\n",
    "print(\"Text features created successfully!\")\n",
    "df_with_text.select(['product_name', 'ingredient_count', 'contains_gluten', 'contains_milk']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4e9b0",
   "metadata": {},
   "source": [
    "## Nutritional Feature Engineering\n",
    "\n",
    "Create nutritional ratios and health scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nutritional_features(df):\n",
    "    \"\"\"Create nutritional features\"\"\"\n",
    "    print(\"Creating nutritional features...\")\n",
    "    \n",
    "    # Energy ratios\n",
    "    if 'energy_100g' in df.columns and 'fat_100g' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'fat_energy_ratio',\n",
    "            F.when(F.col('energy_100g') > 0, \n",
    "                  (F.col('fat_100g') * 9) / F.col('energy_100g')).otherwise(0)\n",
    "        )\n",
    "    \n",
    "    if 'carbohydrates_100g' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'carb_energy_ratio',\n",
    "            F.when(F.col('energy_100g') > 0,\n",
    "                  (F.col('carbohydrates_100g') * 4) / F.col('energy_100g')).otherwise(0)\n",
    "        )\n",
    "    \n",
    "    # Health score based on nutriscore\n",
    "    if 'nutriscore_grade' in df.columns:\n",
    "        nutriscore_map = {'a': 5, 'b': 4, 'c': 3, 'd': 2, 'e': 1}\n",
    "        nutriscore_udf = udf(lambda x: nutriscore_map.get(x.lower() if x else None, 0), IntegerType())\n",
    "        df = df.withColumn('health_score', nutriscore_udf(F.col('nutriscore_grade')))\n",
    "    \n",
    "    # Energy category\n",
    "    if 'energy_100g' in df.columns:\n",
    "        energy_buckets = [-float('inf'), 100, 300, 500, float('inf')]\n",
    "        bucketizer = Bucketizer(splits=energy_buckets, inputCol='energy_100g', outputCol='energy_category')\n",
    "        df = bucketizer.transform(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply nutritional features\n",
    "df_with_nutrition = create_nutritional_features(df_with_text)\n",
    "print(\"Nutritional features created successfully!\")\n",
    "df_with_nutrition.select(['product_name', 'health_score', 'fat_energy_ratio', 'energy_category']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "print(\"\\nApplying feature scaling...\")\n",
    "\n",
    "# Get numerical columns\n",
    "numerical_cols = []\n",
    "for col_name, dtype in df_with_nutrition.dtypes:\n",
    "    if dtype in ['int', 'bigint', 'float', 'double'] and 'id' not in col_name.lower():\n",
    "        # Check for non-null values\n",
    "        non_null_count = df_with_nutrition.filter(F.col(col_name).isNotNull()).count()\n",
    "        if non_null_count > 0:\n",
    "            numerical_cols.append(col_name)\n",
    "\n",
    "print(f\"Found {len(numerical_cols)} numerical columns for scaling\")\n",
    "\n",
    "if len(numerical_cols) >= 2:\n",
    "    # Create feature vector and scale\n",
    "    assembler = VectorAssembler(inputCols=numerical_cols[:10], outputCol=\"features\")\n",
    "    scaler = SparkScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    model = pipeline.fit(df_with_nutrition)\n",
    "    df_final = model.transform(df_with_nutrition)\n",
    "    \n",
    "    print(\"Feature scaling completed!\")\n",
    "else:\n",
    "    df_final = df_with_nutrition\n",
    "    print(\"Insufficient numerical features for scaling\")\n",
    "\n",
    "print(f\"Final dataset: {df_final.count():,} rows, {len(df_final.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save engineered features\n",
    "print(\"\\nSaving engineered features...\")\n",
    "\n",
    "try:\n",
    "    # Save as CSV sample (memory efficient)\n",
    "    sample_df = df_final.limit(50000).toPandas()\n",
    "    output_path = '../data/engineered_features_updated.csv'\n",
    "    sample_df.to_csv(output_path, index=False)\n",
    "    print(f\"Sample features saved to: {output_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'spark_version': spark.version,\n",
    "        'total_rows': df_final.count(),\n",
    "        'total_features': len(df_final.columns),\n",
    "        'new_features': [\n",
    "            'ingredients_filtered', 'ingredient_count', 'category_count',\n",
    "            'contains_gluten', 'contains_milk', 'contains_eggs', 'contains_nuts',\n",
    "            'fat_energy_ratio', 'carb_energy_ratio', 'health_score', 'energy_category'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open('../data/feature_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"Feature engineering completed successfully!\")\n",
    "    print(f\"Created {len(metadata['new_features'])} new features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving: {e}\")\n",
    "\n",
    "# Cleanup\n",
    "print(\"\\nFeature engineering pipeline completed.\")\n",
    "print(\"Run 'spark.stop()' when finished to free resources.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
