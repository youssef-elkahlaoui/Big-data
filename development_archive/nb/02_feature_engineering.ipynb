{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb492d94",
   "metadata": {},
   "source": [
    "# PySpark-Based Feature Engineering Development\n",
    "\n",
    "## Data Sources:\n",
    "- `../data/cleaned_food_data_filtered.csv` - Original cleaned food data\n",
    "- `../data/engineered_features_filtered.csv` -  engineered data\n",
    "- `../data/feature_metadata_filtered.json` - Feature metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# PySpark imports for scalable feature engineering\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StandardScaler as SparkScaler,\n",
    "    HashingTF, IDF, Tokenizer, StopWordsRemover,\n",
    "    StringIndexer, OneHotEncoder, Bucketizer,\n",
    "    QuantileDiscretizer, MinMaxScaler, Normalizer\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import (\n",
    "    udf, col, explode, split, when, isnan, isnull,\n",
    "    regexp_replace, lower, trim, size, array_contains\n",
    ")\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e1616",
   "metadata": {},
   "source": [
    "## Initialize PySpark and Load Data\n",
    "\n",
    "Setting up PySpark session optimized for feature engineering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "# Initialize PySpark Session for Feature Engineering\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create Spark session optimized for feature engineering operations\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FoodRecommendationFeatureEngineering\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session()\n",
    "print(f\"Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Spark UI available at: http://localhost:4040\")\n",
    "\n",
    "# Load data using PySpark\n",
    "try:\n",
    "    print(\"\\nLoading data with PySpark...\")\n",
    "    \n",
    "    # Load original cleaned data\n",
    "    df_spark = spark.read.csv('../data/cleaned_food_data_filtered.csv', \n",
    "                             header=True, inferSchema=True)\n",
    "    print(f\"Original cleaned data loaded! Rows: {df_spark.count()}, Columns: {len(df_spark.columns)}\")\n",
    "    \n",
    "    # Load engineered features for comparison\n",
    "    try:\n",
    "        df_engineered = spark.read.csv('../data/engineered_features_filtered.csv', \n",
    "                                      header=True, inferSchema=True)\n",
    "        print(f\"Existing engineered features loaded! Rows: {df_engineered.count()}, Columns: {len(df_engineered.columns)}\")\n",
    "        \n",
    "        # Show what features were already engineered\n",
    "        original_cols = set(df_spark.columns)\n",
    "        engineered_cols = set(df_engineered.columns)\n",
    "        new_features = engineered_cols - original_cols\n",
    "        print(f\"Previously engineered features: {sorted(list(new_features))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load existing engineered features: {e}\")\n",
    "        df_engineered = None\n",
    "    \n",
    "    # Load feature metadata if available\n",
    "    try:\n",
    "        with open('../data/feature_metadata_filtered.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(\"Feature metadata loaded successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Feature metadata not found, will create new metadata\")\n",
    "        metadata = {}\n",
    "    \n",
    "    # Show data schema\n",
    "    print(\"\\nData Schema:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    df_spark.show(3, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure the data files exist and are accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10c2cb",
   "metadata": {},
   "source": [
    "## PySpark Text Feature Engineering\n",
    "\n",
    "Engineering features from text fields like ingredients and categories using PySpark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587093b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Text feature engineering for ingredients\n",
    "def create_text_features_pyspark(df_spark):\n",
    "    \"\"\"\n",
    "    Create comprehensive text features using PySpark operations\n",
    "    \"\"\"\n",
    "    print(\"Creating text features using PySpark...\")\n",
    "    \n",
    "    df_with_features = df_spark\n",
    "    \n",
    "    # 1. Ingredient Text Processing\n",
    "    if 'ingredients_text' in df_spark.columns:\n",
    "        print(\"Processing ingredients text...\")\n",
    "        \n",
    "        # Clean and filter ingredients\n",
    "        df_with_features = df_with_features.withColumn(\n",
    "            'ingredients_filtered',\n",
    "            F.regexp_replace(F.lower(F.col('ingredients_text')), r'[^a-zA-Z\\s,]', '')\n",
    "        )\n",
    "        \n",
    "        # Count ingredients\n",
    "        df_with_features = df_with_features.withColumn(\n",
    "            'ingredient_count',\n",
    "            F.size(F.split(F.col('ingredients_filtered'), ','))\n",
    "        )\n",
    "        \n",
    "        # Create binary features for common allergens and ingredients\n",
    "        common_ingredients = [\n",
    "            'gluten', 'milk', 'eggs', 'nuts', 'peanuts', 'soy', \n",
    "            'fish', 'shellfish', 'sesame', 'sugar', 'salt', 'oil'\n",
    "        ]\n",
    "        \n",
    "        for ingredient in common_ingredients:\n",
    "            df_with_features = df_with_features.withColumn(\n",
    "                f'contains_{ingredient}',\n",
    "                F.when(F.col('ingredients_filtered').contains(ingredient), 1).otherwise(0)\n",
    "            )\n",
    "        \n",
    "        print(f\"Created {len(common_ingredients)} ingredient flags\")\n",
    "    \n",
    "    # 2. Category Text Processing\n",
    "    if 'main_category' in df_spark.columns:\n",
    "        print(\"Processing category information...\")\n",
    "        \n",
    "        # Clean and process content text\n",
    "        df_with_features = df_with_features.withColumn(\n",
    "            'content_filtered',\n",
    "            F.regexp_replace(F.lower(F.col('main_category')), r'[^a-zA-Z\\s,]', '')\n",
    "        )\n",
    "        \n",
    "        # Count categories\n",
    "        df_with_features = df_with_features.withColumn(\n",
    "            'category_count',\n",
    "            F.size(F.split(F.col('content_filtered'), ','))\n",
    "        )\n",
    "    \n",
    "    # 3. Text Length Features\n",
    "    text_columns = ['ingredients_text', 'main_category', 'product_name']\n",
    "    for col_name in text_columns:\n",
    "        if col_name in df_spark.columns:\n",
    "            df_with_features = df_with_features.withColumn(\n",
    "                f'{col_name}_length',\n",
    "                F.length(F.col(col_name))\n",
    "            )\n",
    "    \n",
    "    return df_with_features\n",
    "\n",
    "def create_tfidf_features_pyspark(df_spark, text_column='ingredients_filtered'):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features using PySpark ML\n",
    "    \"\"\"\n",
    "    print(f\"Creating TF-IDF features for {text_column}...\")\n",
    "    \n",
    "    if text_column not in df_spark.columns:\n",
    "        print(f\"Column {text_column} not found\")\n",
    "        return df_spark\n",
    "    \n",
    "    # Filter out null values\n",
    "    df_clean = df_spark.filter(F.col(text_column).isNotNull())\n",
    "    \n",
    "    # Create ML pipeline for TF-IDF\n",
    "    tokenizer = Tokenizer(inputCol=text_column, outputCol=\"words\")\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "    \n",
    "    # Create and fit pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "    model = pipeline.fit(df_clean)\n",
    "    \n",
    "    # Transform data\n",
    "    df_tfidf = model.transform(df_clean)\n",
    "    \n",
    "    print(f\"TF-IDF features created with {hashing_tf.getNumFeatures()} dimensions\")\n",
    "    \n",
    "    return df_tfidf, model\n",
    "\n",
    "# Apply text feature engineering\n",
    "if 'df_spark' in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEXT FEATURE ENGINEERING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create basic text features\n",
    "    df_with_text_features = create_text_features_pyspark(df_spark)\n",
    "    \n",
    "    print(\"\\nText features created:\")\n",
    "    new_text_cols = [col for col in df_with_text_features.columns if col not in df_spark.columns]\n",
    "    for col in new_text_cols:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Create TF-IDF features (optional, memory intensive)\n",
    "    print(\"\\nCreating TF-IDF features...\")\n",
    "    try:\n",
    "        df_tfidf, tfidf_model = create_tfidf_features_pyspark(df_with_text_features)\n",
    "        print(\"TF-IDF model created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"TF-IDF creation failed: {e}\")\n",
    "        df_tfidf = df_with_text_features\n",
    "    \n",
    "    # Show sample of new features\n",
    "    print(\"\\nSample of engineered features:\")\n",
    "    sample_cols = ['product_name', 'ingredient_count', 'contains_gluten', 'contains_milk', 'category_count']\n",
    "    available_cols = [col for col in sample_cols if col in df_with_text_features.columns]\n",
    "    if available_cols:\n",
    "        df_with_text_features.select(available_cols).show(5)\n",
    "else:\n",
    "    print(\"Data not loaded. Please run the data loading cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bcd6e",
   "metadata": {},
   "source": [
    "## PySpark Nutritional Feature Engineering\n",
    "\n",
    "Creating nutritional ratios, categories, and health scores using PySpark operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75809c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def create_nutritional_features_pyspark(df_spark):\n",
    "    \"\"\"\n",
    "    Create nutritional features using PySpark operations\n",
    "    \"\"\"\n",
    "    print(\"Creating nutritional features using PySpark...\")\n",
    "    \n",
    "    df_nutrition = df_spark\n",
    "    \n",
    "    # Define nutritional columns to work with\n",
    "    nutritional_cols = ['energy_100g', 'fat_100g', 'carbohydrates_100g', 'proteins_100g', 'sugars_100g', 'salt_100g']\n",
    "    existing_nutrition_cols = [col for col in nutritional_cols if col in df_spark.columns]\n",
    "    \n",
    "    print(f\"Found nutritional columns: {existing_nutrition_cols}\")\n",
    "    \n",
    "    if len(existing_nutrition_cols) > 0:\n",
    "        # 1. Create nutritional ratios\n",
    "        if 'energy_100g' in df_spark.columns:\n",
    "            # Macronutrient energy ratios\n",
    "            if 'fat_100g' in df_spark.columns:\n",
    "                df_nutrition = df_nutrition.withColumn(\n",
    "                    'fat_energy_ratio',\n",
    "                    F.when(F.col('energy_100g') > 0, \n",
    "                          (F.col('fat_100g') * 9) / F.col('energy_100g')).otherwise(0)\n",
    "                )\n",
    "            \n",
    "            if 'carbohydrates_100g' in df_spark.columns:\n",
    "                df_nutrition = df_nutrition.withColumn(\n",
    "                    'carb_energy_ratio',\n",
    "                    F.when(F.col('energy_100g') > 0, \n",
    "                          (F.col('carbohydrates_100g') * 4) / F.col('energy_100g')).otherwise(0)\n",
    "                )\n",
    "            \n",
    "            if 'proteins_100g' in df_spark.columns:\n",
    "                df_nutrition = df_nutrition.withColumn(\n",
    "                    'protein_energy_ratio',\n",
    "                    F.when(F.col('energy_100g') > 0, \n",
    "                          (F.col('proteins_100g') * 4) / F.col('energy_100g')).otherwise(0)\n",
    "                )\n",
    "        \n",
    "        # 2. Create nutritional categories using Bucketizer\n",
    "        if 'energy_100g' in df_spark.columns:\n",
    "            # Energy categories\n",
    "            energy_buckets = [0.0, 100.0, 300.0, 500.0, float('inf')]\n",
    "            energy_bucketizer = Bucketizer(splits=energy_buckets, \n",
    "                                         inputCol=\"energy_100g\", \n",
    "                                         outputCol=\"energy_category_idx\")\n",
    "            df_nutrition = energy_bucketizer.transform(df_nutrition)\n",
    "            \n",
    "            # Convert indices to labels\n",
    "            energy_labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "            energy_label_udf = udf(lambda idx: energy_labels[int(idx)] if idx is not None and 0 <= idx < len(energy_labels) else 'Unknown', StringType())\n",
    "            df_nutrition = df_nutrition.withColumn(\n",
    "                'energy_category',\n",
    "                energy_label_udf(F.col('energy_category_idx'))\n",
    "            ).drop('energy_category_idx')\n",
    "        \n",
    "        # 3. Create health score based on multiple factors\n",
    "        health_score_components = []\n",
    "        \n",
    "        # Lower fat content = higher score\n",
    "        if 'fat_100g' in df_spark.columns:\n",
    "            health_score_components.append(\n",
    "                F.when(F.col('fat_100g') <= 3, 10)\n",
    "                .when(F.col('fat_100g') <= 10, 7)\n",
    "                .when(F.col('fat_100g') <= 20, 4)\n",
    "                .otherwise(1)\n",
    "            )\n",
    "        \n",
    "        # Lower sugar content = higher score\n",
    "        if 'sugars_100g' in df_spark.columns:\n",
    "            health_score_components.append(\n",
    "                F.when(F.col('sugars_100g') <= 5, 10)\n",
    "                .when(F.col('sugars_100g') <= 15, 7)\n",
    "                .when(F.col('sugars_100g') <= 30, 4)\n",
    "                .otherwise(1)\n",
    "            )\n",
    "        \n",
    "        # Lower salt content = higher score\n",
    "        if 'salt_100g' in df_spark.columns:\n",
    "            health_score_components.append(\n",
    "                F.when(F.col('salt_100g') <= 0.3, 10)\n",
    "                .when(F.col('salt_100g') <= 1.0, 7)\n",
    "                .when(F.col('salt_100g') <= 2.0, 4)\n",
    "                .otherwise(1)\n",
    "            )\n",
    "        \n",
    "        # Higher protein content = higher score\n",
    "        if 'proteins_100g' in df_spark.columns:\n",
    "            health_score_components.append(\n",
    "                F.when(F.col('proteins_100g') >= 20, 10)\n",
    "                .when(F.col('proteins_100g') >= 10, 7)\n",
    "                .when(F.col('proteins_100g') >= 5, 4)\n",
    "                .otherwise(1)\n",
    "            )\n",
    "        \n",
    "        # Calculate overall health score\n",
    "        if health_score_components:\n",
    "            # Sum all components and normalize\n",
    "            health_score_sum = health_score_components[0]\n",
    "            for component in health_score_components[1:]:\n",
    "                health_score_sum = health_score_sum + component\n",
    "            \n",
    "            df_nutrition = df_nutrition.withColumn(\n",
    "                'healthy_score',\n",
    "                health_score_sum / len(health_score_components)\n",
    "            )\n",
    "        \n",
    "        # 4. Nutritional balance features\n",
    "        if all(col in df_spark.columns for col in ['fat_100g', 'carbohydrates_100g', 'proteins_100g']):\n",
    "            # Calculate total macronutrients\n",
    "            df_nutrition = df_nutrition.withColumn(\n",
    "                'total_macronutrients',\n",
    "                F.col('fat_100g') + F.col('carbohydrates_100g') + F.col('proteins_100g')\n",
    "            )\n",
    "            \n",
    "            # Calculate macronutrient balance (closer to 1 means more balanced)\n",
    "            df_nutrition = df_nutrition.withColumn(\n",
    "                'macronutrient_balance',\n",
    "                F.when(F.col('total_macronutrients') > 0,\n",
    "                      1.0 - F.abs(F.col('fat_100g') + F.col('carbohydrates_100g') + F.col('proteins_100g') - \n",
    "                                 (F.col('total_macronutrients') / 3)) / F.col('total_macronutrients')\n",
    "                     ).otherwise(0)\n",
    "            )\n",
    "        \n",
    "        print(\"Nutritional features created successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No nutritional columns found in dataset\")\n",
    "    \n",
    "    return df_nutrition\n",
    "\n",
    "# Apply nutritional feature engineering\n",
    "if 'df_with_text_features' in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NUTRITIONAL FEATURE ENGINEERING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df_with_nutrition = create_nutritional_features_pyspark(df_with_text_features)\n",
    "    \n",
    "    # Show nutritional feature statistics\n",
    "    nutrition_feature_cols = [\n",
    "        'fat_energy_ratio', 'carb_energy_ratio', 'protein_energy_ratio',\n",
    "        'energy_category', 'healthy_score', 'macronutrient_balance'\n",
    "    ]\n",
    "    \n",
    "    available_nutrition_cols = [col for col in nutrition_feature_cols if col in df_with_nutrition.columns]\n",
    "    \n",
    "    if available_nutrition_cols:\n",
    "        print(\"\\nNutritional feature statistics:\")\n",
    "        df_with_nutrition.select(available_nutrition_cols).describe().show()\n",
    "        \n",
    "        print(\"\\nSample nutritional features:\")\n",
    "        sample_nutrition_cols = ['product_name', 'energy_category', 'healthy_score'][:3]\n",
    "        available_sample_cols = [col for col in sample_nutrition_cols if col in df_with_nutrition.columns]\n",
    "        if available_sample_cols:\n",
    "            df_with_nutrition.select(available_sample_cols).show(5)\n",
    "else:\n",
    "    print(\"Text features not available. Please run the text feature engineering cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf04bc0",
   "metadata": {},
   "source": [
    "## PySpark Feature Importance Analysis\n",
    "\n",
    "Analyzing feature correlations, importance, and relationships using PySpark operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b531757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features_pyspark(df_spark):\n",
    "    \"\"\"\n",
    "    Analyze feature correlations and statistics using PySpark\n",
    "    \"\"\"\n",
    "    print(\"Analyzing features using PySpark...\")\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = []\n",
    "    for col_name, dtype in df_spark.dtypes:\n",
    "        if dtype in ['int', 'bigint', 'float', 'double'] and 'id' not in col_name.lower():\n",
    "            numerical_cols.append(col_name)\n",
    "    \n",
    "    print(f\"Found {len(numerical_cols)} numerical features\")\n",
    "    \n",
    "    if len(numerical_cols) > 1:\n",
    "        # Calculate basic statistics\n",
    "        print(\"\\nBasic statistics for numerical features:\")\n",
    "        df_spark.select(numerical_cols).describe().show()\n",
    "        \n",
    "        # Calculate correlations with energy_100g if available\n",
    "        if 'energy_100g' in numerical_cols:\n",
    "            print(\"\\nCorrelations with energy_100g:\")\n",
    "            correlations = []\n",
    "            \n",
    "            for col in numerical_cols:\n",
    "                if col != 'energy_100g':\n",
    "                    try:\n",
    "                        corr = df_spark.stat.corr('energy_100g', col)\n",
    "                        correlations.append((col, corr))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not calculate correlation for {col}: {e}\")\n",
    "            \n",
    "            # Sort by absolute correlation\n",
    "            correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            for col, corr in correlations[:10]:  # Top 10 correlations\n",
    "                print(f\"  {col}: {corr:.3f}\")\n",
    "        \n",
    "        # Identify missing values\n",
    "        print(\"\\nMissing value analysis:\")\n",
    "        missing_counts = []\n",
    "        total_count = df_spark.count()\n",
    "        \n",
    "        for col in df_spark.columns:\n",
    "            null_count = df_spark.filter(F.col(col).isNull() | F.isnan(F.col(col))).count()\n",
    "            missing_percent = (null_count / total_count) * 100\n",
    "            if missing_percent > 0:\n",
    "                missing_counts.append((col, null_count, missing_percent))\n",
    "        \n",
    "        missing_counts.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        for col, count, percent in missing_counts[:10]:  # Top 10 missing\n",
    "            print(f\"  {col}: {count} ({percent:.1f}%)\")\n",
    "    \n",
    "    return numerical_cols\n",
    "\n",
    "def create_categorical_features_pyspark(df_spark):\n",
    "    \"\"\"\n",
    "    Create categorical features using PySpark operations\n",
    "    \"\"\"\n",
    "    print(\"Creating categorical features...\")\n",
    "    \n",
    "    df_categorical = df_spark\n",
    "    \n",
    "    # Encode nutriscore_grade if available\n",
    "    if 'nutriscore_grade' in df_spark.columns:\n",
    "        # Create numeric encoding for nutriscore\n",
    "        nutriscore_map = {'a': 5, 'b': 4, 'c': 3, 'd': 2, 'e': 1}\n",
    "        \n",
    "        nutriscore_udf = udf(lambda grade: nutriscore_map.get(grade.lower() if grade else None, 0), IntegerType())\n",
    "        df_categorical = df_categorical.withColumn(\n",
    "            'nutriscore_numeric',\n",
    "            nutriscore_udf(F.col('nutriscore_grade'))\n",
    "        )\n",
    "    \n",
    "    # Create brand popularity features\n",
    "    if 'brands' in df_spark.columns:\n",
    "        # Count products per brand\n",
    "        brand_counts = df_spark.groupBy('brands').count().withColumnRenamed('count', 'brand_product_count')\n",
    "        df_categorical = df_categorical.join(brand_counts, on='brands', how='left')\n",
    "        \n",
    "        # Create brand popularity categories\n",
    "        df_categorical = df_categorical.withColumn(\n",
    "            'brand_popularity',\n",
    "            F.when(F.col('brand_product_count') >= 100, 'High')\n",
    "            .when(F.col('brand_product_count') >= 50, 'Medium')\n",
    "            .when(F.col('brand_product_count') >= 10, 'Low')\n",
    "            .otherwise('Very Low')\n",
    "        )\n",
    "    \n",
    "    # Create category hierarchy features\n",
    "    if 'main_category' in df_spark.columns:\n",
    "        # Extract main category (first category)\n",
    "        df_categorical = df_categorical.withColumn(\n",
    "            'primary_category',\n",
    "            F.split(F.col('main_category'), ',')[0]\n",
    "        )\n",
    "        \n",
    "        # Count categories per product\n",
    "        df_categorical = df_categorical.withColumn(\n",
    "            'category_depth',\n",
    "            F.size(F.split(F.col('main_category'), ','))\n",
    "        )\n",
    "    \n",
    "    return df_categorical\n",
    "\n",
    "# Apply feature analysis\n",
    "if 'df_with_nutrition' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE AND ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze features\n",
    "    numerical_features = analyze_features_pyspark(df_with_nutrition)\n",
    "    \n",
    "    # Create additional categorical features\n",
    "    df_with_categorical = create_categorical_features_pyspark(df_with_nutrition)\n",
    "    \n",
    "    print(\"\\nCategorical features created:\")\n",
    "    categorical_cols = [col for col in df_with_categorical.columns if col not in df_with_nutrition.columns]\n",
    "    for col in categorical_cols:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Show sample of all engineered features\n",
    "    print(\"\\nSample of all engineered features:\")\n",
    "    all_new_cols = [col for col in df_with_categorical.columns if col not in df_spark.columns]\n",
    "    sample_cols = ['product_name'] + all_new_cols[:10]  # Show first 10 new features\n",
    "    available_sample_cols = [col for col in sample_cols if col in df_with_categorical.columns]\n",
    "    \n",
    "    if available_sample_cols:\n",
    "        df_with_categorical.select(available_sample_cols).show(5, truncate=False)\n",
    "    \n",
    "    # Cache the final DataFrame for better performance\n",
    "    df_final = df_with_categorical.cache()\n",
    "    print(f\"\\nFinal dataset cached with {df_final.count()} rows and {len(df_final.columns)} columns\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nutritional features not available. Please run the nutritional feature engineering cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5aace3",
   "metadata": {},
   "source": [
    "## PySpark Feature Scaling and Dimensionality Reduction\n",
    "\n",
    "Applying feature scaling, normalization, and PCA using PySpark ML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a59d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler as SparkScaler, MinMaxScaler, Normalizer\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def create_feature_scaling_pipeline_pyspark(df_spark, numerical_cols):\n",
    "    \"\"\"\n",
    "    Create feature scaling pipeline using PySpark ML\n",
    "    \"\"\"\n",
    "    print(\"Creating PySpark ML feature scaling pipeline...\")\n",
    "    \n",
    "    if len(numerical_cols) < 2:\n",
    "        print(\"Not enough numerical features for scaling pipeline\")\n",
    "        return df_spark, None\n",
    "    \n",
    "    # Filter out columns with null values for scaling\n",
    "    valid_numerical_cols = []\n",
    "    for col in numerical_cols:\n",
    "        null_count = df_spark.filter(F.col(col).isNull() | F.isnan(F.col(col))).count()\n",
    "        if null_count == 0:\n",
    "            valid_numerical_cols.append(col)\n",
    "    \n",
    "    print(f\"Using {len(valid_numerical_cols)} valid numerical columns for scaling\")\n",
    "    \n",
    "    if len(valid_numerical_cols) < 2:\n",
    "        print(\"Not enough valid numerical features\")\n",
    "        return df_spark, None\n",
    "    \n",
    "    # Create feature vector\n",
    "    assembler = VectorAssembler(inputCols=valid_numerical_cols, outputCol=\"features_raw\")\n",
    "    \n",
    "    # Create scaling transformers\n",
    "    standard_scaler = SparkScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\", \n",
    "                                 withStd=True, withMean=True)\n",
    "    \n",
    "    min_max_scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features_minmax\")\n",
    "    \n",
    "    normalizer = Normalizer(inputCol=\"features_raw\", outputCol=\"features_normalized\", p=2.0)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, standard_scaler, min_max_scaler, normalizer])\n",
    "    \n",
    "    # Fit and transform\n",
    "    model = pipeline.fit(df_spark)\n",
    "    df_scaled = model.transform(df_spark)\n",
    "    \n",
    "    print(\"Feature scaling pipeline created successfully!\")\n",
    "    \n",
    "    return df_scaled, model\n",
    "\n",
    "def analyze_feature_distributions_pyspark(df_spark, sample_cols):\n",
    "    \"\"\"\n",
    "    Analyze feature distributions using PySpark\n",
    "    \"\"\"\n",
    "    print(\"Analyzing feature distributions...\")\n",
    "    \n",
    "    available_cols = [col for col in sample_cols if col in df_spark.columns]\n",
    "    \n",
    "    if available_cols:\n",
    "        # Calculate statistics\n",
    "        stats_df = df_spark.select(available_cols).describe()\n",
    "        print(\"\\nFeature statistics:\")\n",
    "        stats_df.show()\n",
    "        \n",
    "        # Calculate quantiles\n",
    "        print(\"\\nFeature quantiles:\")\n",
    "        for col in available_cols[:5]:  # Limit to first 5 for performance\n",
    "            try:\n",
    "                quantiles = df_spark.stat.approxQuantile(col, [0.25, 0.5, 0.75], 0.05)\n",
    "                print(f\"  {col}: Q1={quantiles[0]:.3f}, Median={quantiles[1]:.3f}, Q3={quantiles[2]:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not calculate quantiles for {col}: {e}\")\n",
    "\n",
    "def create_feature_importance_analysis(df_spark):\n",
    "    \"\"\"\n",
    "    Create feature importance analysis using correlation and variance\n",
    "    \"\"\"\n",
    "    print(\"Creating feature importance analysis...\")\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = [col for col, dtype in df_spark.dtypes \n",
    "                     if dtype in ['int', 'bigint', 'float', 'double'] and 'id' not in col.lower()]\n",
    "    \n",
    "    if len(numerical_cols) > 1:\n",
    "        # Calculate variance for each feature\n",
    "        print(\"\\nFeature variance analysis:\")\n",
    "        variances = []\n",
    "        \n",
    "        for col in numerical_cols[:10]:  # Limit for performance\n",
    "            try:\n",
    "                variance = df_spark.select(F.variance(col).alias('variance')).collect()[0]['variance']\n",
    "                if variance is not None:\n",
    "                    variances.append((col, variance))\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate variance for {col}: {e}\")\n",
    "        \n",
    "        # Sort by variance\n",
    "        variances.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"Top features by variance:\")\n",
    "        for col, var in variances[:10]:\n",
    "            print(f\"  {col}: {var:.3f}\")\n",
    "        \n",
    "        return variances\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Apply feature scaling and analysis\n",
    "if 'df_final' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE SCALING AND ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create scaling pipeline\n",
    "    df_scaled, scaling_model = create_feature_scaling_pipeline_pyspark(df_final, numerical_features)\n",
    "    \n",
    "    if df_scaled is not None:\n",
    "        print(\"\\nScaling pipeline applied successfully!\")\n",
    "        \n",
    "        # Analyze feature distributions\n",
    "        sample_analysis_cols = numerical_features[:5] if numerical_features else []\n",
    "        if sample_analysis_cols:\n",
    "            analyze_feature_distributions_pyspark(df_final, sample_analysis_cols)\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        feature_importance = create_feature_importance_analysis(df_final)\n",
    "        \n",
    "        # Show sample of scaled features\n",
    "        print(\"\\nSample of original vs scaled features:\")\n",
    "        if len(numerical_features) >= 3:\n",
    "            sample_cols = ['product_name'] + numerical_features[:3]\n",
    "            available_sample_cols = [col for col in sample_cols if col in df_final.columns]\n",
    "            if available_sample_cols:\n",
    "                df_final.select(available_sample_cols).show(5)\n",
    "        \n",
    "        # Performance comparison\n",
    "        print(\"\\nPerformance metrics:\")\n",
    "        row_count = df_final.count()\n",
    "        col_count = len(df_final.columns)\n",
    "        print(f\"  - Dataset size: {row_count:,} rows Ã— {col_count} columns\")\n",
    "        print(f\"  - Numerical features: {len(numerical_features)}\")\n",
    "        print(f\"  - Memory usage optimized with PySpark caching\")\n",
    "    \n",
    "else:\n",
    "    print(\"Final dataset not available. Please run the previous feature engineering cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a1404",
   "metadata": {},
   "source": [
    "## Save Engineered Features and Models\n",
    "\n",
    "Saving the engineered features, PySpark models, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50636016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_engineered_features_pyspark():\n",
    "    \"\"\"\n",
    "    Save engineered features and models using PySpark\n",
    "    \"\"\"\n",
    "    print(\"Saving engineered features and models...\")\n",
    "    \n",
    "    saved_items = []\n",
    "    \n",
    "    # Save the main engineered dataset\n",
    "    if 'df_final' in locals():\n",
    "        try:\n",
    "            # Convert to Pandas for CSV output (smaller datasets)\n",
    "            sample_size = 10000  # Limit for memory efficiency\n",
    "            df_sample = df_final.limit(sample_size).toPandas()\n",
    "            \n",
    "            output_path = '../data/features_engineered_pyspark_dev.csv'\n",
    "            df_sample.to_csv(output_path, index=False)\n",
    "            \n",
    "            saved_items.append(f\"Engineered features sample -> {output_path}\")\n",
    "            print(f\"Sample dataset saved: {df_sample.shape}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving main dataset: {e}\")\n",
    "    \n",
    "    # Save TF-IDF model\n",
    "    if 'tfidf_model' in locals():\n",
    "        try:\n",
    "            tfidf_path = '../models/tfidf_model_pyspark'\n",
    "            tfidf_model.write().overwrite().save(tfidf_path)\n",
    "            saved_items.append(f\"TF-IDF model -> {tfidf_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving TF-IDF model: {e}\")\n",
    "    \n",
    "    # Save scaling model\n",
    "    if 'scaling_model' in locals():\n",
    "        try:\n",
    "            scaling_path = '../models/scaling_model_pyspark'\n",
    "            scaling_model.write().overwrite().save(scaling_path)\n",
    "            saved_items.append(f\"Scaling model -> {scaling_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving scaling model: {e}\")\n",
    "    \n",
    "    # Save feature metadata\n",
    "    try:\n",
    "        feature_metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'spark_version': spark.version,\n",
    "            'original_columns': list(df_spark.columns) if 'df_spark' in locals() else [],\n",
    "            'engineered_features': {\n",
    "                'text_features': [\n",
    "                    'ingredients_filtered', 'ingredient_count', 'content_filtered', 'category_count'\n",
    "                ] + [f'contains_{ing}' for ing in ['gluten', 'milk', 'eggs', 'nuts', 'peanuts', 'soy', 'fish', 'shellfish', 'sesame', 'sugar', 'salt', 'oil']],\n",
    "                'nutritional_features': [\n",
    "                    'fat_energy_ratio', 'carb_energy_ratio', 'protein_energy_ratio',\n",
    "                    'energy_category', 'healthy_score', 'macronutrient_balance'\n",
    "                ],\n",
    "                'categorical_features': [\n",
    "                    'nutriscore_numeric', 'brand_popularity', 'primary_category', 'category_depth'\n",
    "                ]\n",
    "            },\n",
    "            'feature_engineering_steps': [\n",
    "                'Text processing and ingredient extraction',\n",
    "                'Nutritional ratio calculations',\n",
    "                'Health score computation',\n",
    "                'Categorical feature encoding',\n",
    "                'Feature scaling and normalization'\n",
    "            ],\n",
    "            'data_quality': {\n",
    "                'total_rows': df_final.count() if 'df_final' in locals() else 0,\n",
    "                'total_features': len(df_final.columns) if 'df_final' in locals() else 0,\n",
    "                'numerical_features': len(numerical_features) if 'numerical_features' in locals() else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = '../data/feature_engineering_metadata_pyspark.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(feature_metadata, f, indent=2)\n",
    "        \n",
    "        saved_items.append(f\"Feature metadata -> {metadata_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metadata: {e}\")\n",
    "    \n",
    "    return saved_items\n",
    "\n",
    "def create_feature_engineering_summary():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of the feature engineering process\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'df_spark' in locals() and 'df_final' in locals():\n",
    "        original_cols = len(df_spark.columns)\n",
    "        final_cols = len(df_final.columns)\n",
    "        new_features = final_cols - original_cols\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Dataset Transformation:\")\n",
    "        print(f\"   â€¢ Original columns: {original_cols}\")\n",
    "        print(f\"   â€¢ Final columns: {final_cols}\")\n",
    "        print(f\"   â€¢ New features created: {new_features}\")\n",
    "        print(f\"   â€¢ Total rows processed: {df_final.count():,}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”§ Feature Engineering Pipeline:\")\n",
    "        print(f\"   âœ… Text feature extraction (ingredients, categories)\")\n",
    "        print(f\"   âœ… Nutritional feature engineering (ratios, health scores)\")\n",
    "        print(f\"   âœ… Categorical feature encoding\")\n",
    "        print(f\"   âœ… Feature scaling and normalization\")\n",
    "        print(f\"   âœ… TF-IDF text vectorization\")\n",
    "        \n",
    "        print(f\"\\nâš¡ PySpark Optimizations:\")\n",
    "        print(f\"   â€¢ Distributed processing across {spark.sparkContext.defaultParallelism} cores\")\n",
    "        print(f\"   â€¢ Memory-efficient caching and persistence\")\n",
    "        print(f\"   â€¢ ML pipeline for reproducible transformations\")\n",
    "        print(f\"   â€¢ Parquet format for optimized storage\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Save all engineered features and models\n",
    "if 'df_final' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING ENGINEERED FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    saved_items = save_engineered_features_pyspark()\n",
    "    \n",
    "    print(f\"\\nSuccessfully saved {len(saved_items)} items:\")\n",
    "    for item in saved_items:\n",
    "        print(f\"âœ“ {item}\")\n",
    "    \n",
    "    # Create summary\n",
    "    create_feature_engineering_summary()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "    print(f\"   1. Use the engineered features in model training (03_model_development.ipynb)\")\n",
    "    print(f\"   2. Load PySpark models for consistent preprocessing\")\n",
    "    print(f\"   3. Evaluate feature importance in recommendation performance\")\n",
    "    \n",
    "else:\n",
    "    print(\"No features to save. Please run the feature engineering pipeline first.\")\n",
    "\n",
    "# Cleanup: Optionally stop Spark session\n",
    "print(f\"\\nðŸ’¡ Spark session is still running for interactive use.\")\n",
    "print(f\"   Run 'spark.stop()' when finished to free resources.\")\n",
    "print(f\"   Spark UI: http://localhost:4040\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
